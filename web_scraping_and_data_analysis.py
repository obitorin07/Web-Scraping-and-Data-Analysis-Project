# -*- coding: utf-8 -*-
"""web_scraping_and_data_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dpbHUDz_QB1cZe6g0WA4uqyIVr5SdPBg
"""

# !pip install bs4 ydata-profiling python-dotenv

# ! echo usr ='xxxxxxxxxxxxxxxxxxx' > credential.env                                         #removed credentials

# !cat credential.env

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
import requests
from bs4 import BeautifulSoup
from dotenv import dotenv_values
from ydata_profiling import ProfileReport
import warnings
warnings.filterwarnings('ignore')

"""Send Request to website

Targeted website
* i created dummy website using open source data from kaggle
"""

url = 'https://datawebscrapping.vercel.app/super_store.html'

usr = dotenv_values('/credential.env')

response =  requests.get(url ,{'user_agent':usr['usr']})
response

"""Using bs4 to extract data from website"""

soup =  BeautifulSoup(response.text , 'html.parser')

articles = soup.find_all('div', {'class':'article'})

"""Saving Each Articles in new text files (automatic)"""

count = 0
for i in articles:
    if i:
        with open(f"article{count+1}.txt", 'w') as arti:
            # print(i.text)
            arti.write(i.text)
            count+=1
    j = i.find('h2')
    if j:
        with open('article_title.txt','a') as tit:
            tit.write(j.text+'\n')
print('Process Done')

"""##### Scrap Tables"""

tables = soup.find_all('table',{'class':'dataframe sample_1000_r'})[0]
#tables
#cleared output because when i try to convert this .ipy to a pdf also all output dispaying

columns = tables.find_all('th')
columns

#extracting column /variables from table
column =  [i.text.strip() for i in columns]
column

rows =  tables.find_all('tr')
rows[0]         #checking

row = []
for r in rows[1:]:
    k = r.find_all('td')
    row_data = [i.text.strip() for i in k]
    row.append(row_data)

row[0]

"""Table Scrapping Done lets create dataframe to store these values"""

df = pd.DataFrame(row , columns=column)

df.head()
# table looks good

"""## Data Cleaning/Preprocessing/Cleansing/Transformation

remove un_necessary columns/variables
"""

#making all variables name to lower case  with '_'
df.columns =  df.columns.str.lower().str.replace(" ", "_")

df.drop(inplace =True ,  columns =['row_id', 'postal_code', 'country'] )

df.info()

"""The data is not in the correct format:

1. order_date and ship_date are objects instead of datetime.
2. sales, quantity, and profit should be integers or floats instead of objects.

First Change datetime
"""

#data format 7/20/2017
date_cols = df[['order_date','ship_date']]

for col in date_cols:
    df[col] =df[col].str.replace('/', '-').str.strip()
    df[col] = pd.to_datetime(df[col] ,  format = '%m-%d-%Y')
print('process done')

df.info()

"""Now date variables look good now need to change dtypes for sales, quantity,profit"""

#$69.71 i want to extract only 69.71 either use regex or replaceðŸ˜€

re.findall('\d+\.\d+','$69.71')[0]

df.sales = df.sales.apply(lambda x :float(re.findall('\d+\.\d+',x)[0]))

df.profit = df.profit.apply(lambda x :re.sub(r"[^\d.-]","",x))

df.profit.replace('-',0,inplace =True)

num_col =df[['profit','quantity']]

for i in num_col:
    df[i] = pd.to_numeric(df[i])

df.info()

"""Data Loooking fine now i can do some analysis using this data now"""

#do some quick EDA

report  =  ProfileReport(df)
report.to_file('quick_report.html')

"""# Data Analysis and Visualization

Sales Trends:

* How sales trends changed over time? Are there any noticeable patterns or seasonality?
"""

## these are custom color code i use chatgpt to generate these color code

# Warm Shades
warm_palette = ['#ff9999', '#ff6666', '#ff4d4d', '#ff3333', '#ff0000']

# Cool Blues
cool_blues_palette = ['#cce5ff', '#99c2ff', '#66b3ff', '#3399ff', '#007bff']

# Earth Tones
earth_tones_palette = ['#d9e3d8', '#b8d1c6', '#9ac9b3', '#6cb4a0', '#4e8c6f']

# Sunset Colors
sunset_palette = ['#ffcc99', '#ffb366', '#ff9933', '#ff6600', '#cc3300']

# Pastel Shades
pastel_palette = ['#f7c6c7', '#f7a6a8', '#f78180', '#f75050', '#d72027']

df['month'] = df.order_date.dt.month    # extraing month and year from date feature
df['year'] = df.order_date.dt.year

#int32 to int 64
df[['month','year']] = df[['month','year']].astype('int64')

df.info()

#pivot table
df.pivot_table(index ='year' ,columns= 'month' ,values= 'sales',aggfunc='sum').reset_index()

sales_by_month = df.groupby('month').agg(Sales = ('sales','sum')).reset_index()
sales_by_month

sales_by_year = df.groupby('year').agg(Sales = ('sales','sum')).reset_index()
sales_by_year

"""Sales by Month

"""

plt.figure(figsize=(12, 6))
sns.lineplot(data=sales_by_month, x='month', y='Sales', marker='o', palette='#d9e3d8')
plt.title('Sales Trends by Month')
plt.xlabel('Month')
plt.ylabel('Sales')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""Sales by Year"""

plt.figure(figsize=(12, 5))
sns.lineplot(sales_by_year ,  x = 'year', y ='Sales' , marker ='o', markerfacecolor = 'black')
plt.grid(True)
plt.title('Sales Trend by Year')
plt.xlabel('Year')
plt.ylabel('Sales')

"""Sales Performance by Region:

* Which regions have the highest and lowest sales? How does sales performance vary across different regions?
"""

(df.region.value_counts(normalize=True) * 100).reset_index()

sales_by_region =  df.groupby('region').sales.sum().reset_index().sort_values(by = 'sales', ascending =False)
sales_by_region

profit_by_region =  df.groupby('region').profit.sum().reset_index().sort_values(by = 'profit', ascending =False)
profit_by_region

plt.figure(figsize = (14,5))
plt.subplot(1,2,1)
sns.barplot(sales_by_region, x = 'sales' ,y = 'region', hue = 'sales' ,  palette=cool_blues_palette)
plt.title('Sales By Region')
plt.xlabel('Sales')
plt.ylabel('Region')

plt.subplot(1,2,2)
sns.barplot(profit_by_region, x = 'profit' ,y = 'region', hue = 'profit' ,  palette=cool_blues_palette)
plt.title('Profit By Region')
plt.xlabel('Profit')
plt.ylabel('Region')

"""Customer Segments:

* What are the sales and profit contributions from different customer segments? Which segment is the most profitable?
"""

segment_contribution =df.pivot_table(index = 'segment',  values = ['sales','profit'], aggfunc ='sum' ).reset_index()
segment_contribution

segment_contribution.set_index('segment').plot(kind='bar', stacked=True, figsize=(10,6), color=['#3498db', '#e74c3c'])
plt.title('Sales and Profit by Segment')
plt.xlabel('Segment')
plt.ylabel('Total Value')
plt.xticks(rotation=45)
plt.show()

df.head()

"""category and sub category sales and profit"""

cat_subCat = df.groupby(['category','sub-category']).sales.sum().reset_index().sort_values(by = ['category','sales'],ascending =[True,False ])
sub_cat_subCat =cat_subCat.sort_values(by = 'sales', ascending=False)

x=  df.groupby('category').sales.sum().reset_index()
x

profit_cat =  df.groupby('category').profit.sum().reset_index().sort_values(by ='profit',ascending =False)
profit_cat

profit_Scat =  df.groupby('sub-category').profit.sum().reset_index().sort_values(by ='profit',ascending =False)
profit_Scat

plt.figure(figsize =(28,18)),
plt.subplot(2,2,1)
sns.barplot(x , x ='category', y ='sales')
plt.title('Sales By Category')

plt.subplot(2,2,2)
sns.barplot(sub_cat_subCat , x = 'sub-category', y='sales' ,palette= cool_blues_palette )
plt.xticks(rotation =45 )
plt.title('Sales By Sub-Category')

plt.subplot(2,2,3)
sns.barplot(profit_cat , x = 'category', y='profit' ,palette= cool_blues_palette )
plt.title('Profit By Category')

plt.subplot(2,2,4)
sns.barplot(profit_Scat , x = 'sub-category', y='profit' ,palette= cool_blues_palette )
plt.xticks(rotation =45 )
plt.title('Profit By Sub-Category')

"""### Table scapping without bs4 using pandas

Pandas only scrapes/extracts tables from webpages with less control over data
* In BeautifulSoup (bs4), we can scrape whatever we want, including text, tables, and other content.
* Note: ðŸ˜€ With Authorized Access
"""

medals = pd.read_html('https://en.wikipedia.org/wiki/2024_Summer_Olympics_medal_table')

type(medals)

len(medals)

df = medals[3]

df.head()

"""# **Thank you**ðŸ™ƒ"""

!jupyter nbconvert --execute --to html /content/file_updated.ipynb

# !jupyter nbconvert --execute --to pdf /content/scrap.ipynb

